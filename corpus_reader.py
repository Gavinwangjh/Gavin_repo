# corpus_reader.py
"""
è·¨è®ºæ–‡é—®ç­” / ç»¼è¿°ç”Ÿæˆæ¨¡å—

ä¾èµ–ï¼š
- data/index/faiss.index
- data/index/docs.json
- data/index/metadata.json

è¿™äº›ç”± paper_indexer.py ç”Ÿæˆï¼š
    python paper_indexer.py
"""

import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from llm_api import call_llm

INDEX_DIR = "data/index"

# å’Œä¹‹å‰ä¿æŒä¸€è‡´çš„ embedding æ¨¡å‹
embed_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")


def load_corpus():
    """åŠ è½½ FAISS ç´¢å¼• + æ–‡æœ¬ç‰‡æ®µ docs + è®ºæ–‡å…ƒæ•°æ® metadata"""
    index_path = os.path.join(INDEX_DIR, "faiss.index")
    docs_path = os.path.join(INDEX_DIR, "docs.json")
    meta_path = os.path.join(INDEX_DIR, "metadata.json")

    if not os.path.exists(index_path):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°ç´¢å¼•æ–‡ä»¶: {index_path}ï¼Œè¯·å…ˆè¿è¡Œ paper_indexer.py")

    index = faiss.read_index(index_path)

    with open(docs_path, "r", encoding="utf-8") as f:
        docs = json.load(f)

    with open(meta_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    # å»ºä¸€ä¸ª paper_id -> filename çš„æ˜ å°„ï¼Œæ–¹ä¾¿å±•ç¤º
    pid2fname = {m["paper_id"]: m["filename"] for m in metadata}

    return index, docs, pid2fname


def search_corpus(question: str, index, docs, top_k: int = 10):
    """
    åœ¨â€œæ‰€æœ‰è®ºæ–‡çš„æ‰€æœ‰æ®µè½â€ä¸Šåšè¯­ä¹‰æ£€ç´¢ï¼Œè¿”å› top_k ä¸ªæœ€ç›¸å…³æ®µè½
    è¿”å›ç»“æœæ ¼å¼ï¼š[ {paper_id, page, text, score}, ... ]
    """
    q_vec = embed_model.encode([question], convert_to_numpy=True)
    D, I = index.search(q_vec, top_k)

    results = []
    for dist, idx in zip(D[0], I[0]):
        d = docs[int(idx)]
        d = d.copy()
        d["score"] = float(dist)
        results.append(d)

    return results


def build_corpus_prompt(question: str, hits, pid2fname, max_chars: int = 4000):
    """
    æŠŠå¤šä¸ªè®ºæ–‡ç‰‡æ®µ + é¡µç  + æ–‡ä»¶å æ‹¼æˆä¸€ä¸ªå¤§ promptï¼Œå–‚ç»™ LLM ç”¨ã€‚
    max_chars ç”¨æ¥æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé˜²æ­¢å¤ªé•¿ã€‚
    """
    context_lines = []
    used_evidence = []

    total_len = 0
    for h in hits:
        paper_id = h["paper_id"]
        page = h["page"]
        text = h["text"]
        fname = pid2fname.get(paper_id, paper_id)

        snippet = f"ã€è®ºæ–‡ï¼š{fname} | ç¬¬{page}é¡µã€‘\n{text}\n"
        if total_len + len(snippet) > max_chars:
            break

        context_lines.append(snippet)
        total_len += len(snippet)

        used_evidence.append({
            "paper_id": paper_id,
            "filename": fname,
            "page": page,
            "text": text
        })

    context = "\n".join(context_lines)

    prompt = f"""
ä½ æ˜¯ä¸€åç§‘ç ”è®ºæ–‡ç»¼è¿°åŠ©æ‰‹ã€‚ç°åœ¨æœ‰å¤šç¯‡è®ºæ–‡çš„åŸæ–‡ç‰‡æ®µï¼Œè¯·ä½ åŸºäºè¿™äº›å†…å®¹å›ç­”ä¸€ä¸ªç§‘ç ”é—®é¢˜ã€‚

ä¸‹é¢æ˜¯æ£€ç´¢åˆ°çš„è®ºæ–‡åŸæ–‡ç‰‡æ®µï¼ˆåŒ…å«è®ºæ–‡åä¸é¡µç ï¼‰ï¼š
------------------------
{context}
------------------------

è¦æ±‚ï¼š
1. å›ç­”å¿…é¡»åªåŸºäºä»¥ä¸Šç‰‡æ®µçš„å†…å®¹ï¼Œä¸è¦ç¼–é€ è®ºæ–‡ä¸­æ²¡æœ‰çš„ç»“è®ºã€‚
2. å°½é‡ç”¨æ¡ç†æ¸…æ™°çš„æ–¹å¼æ€»ç»“ï¼ˆå¯ä»¥ç”¨åˆ†ç‚¹ï¼‰ã€‚
3. å¦‚æœä¸åŒè®ºæ–‡æœ‰ä¸åŒè§‚ç‚¹ï¼Œå¯ä»¥æŒ‡å‡ºå·®å¼‚ã€‚
4. æœ€åç»™å‡ºä¸€å¥ç®€çŸ­æ€»ç»“ã€‚

é—®é¢˜ï¼š{question}
"""
    return prompt, used_evidence


def corpus_rag_query(question: str, top_k: int = 10, max_chars: int = 4000):
    """
    å¯¹â€œæ•´ä¸ªè®ºæ–‡è¯­æ–™åº“â€è¿›è¡Œ RAG é—®ç­”ï¼š
    è¾“å…¥ï¼šé—®é¢˜ï¼ˆä¸­æ–‡æˆ–è‹±æ–‡ï¼‰
    è¾“å‡ºï¼š
        answer: LLM ç”Ÿæˆçš„æ€»ç»“
        evidence: ç”¨åˆ°çš„è®ºæ–‡ç‰‡æ®µåˆ—è¡¨ï¼ˆå« paper_id / filename / page / textï¼‰
    """
    index, docs, pid2fname = load_corpus()

    hits = search_corpus(question, index, docs, top_k=top_k)
    if not hits:
        return "æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œè¯·æ¢ä¸€ä¸ªé—®é¢˜è¯•è¯•ã€‚", []

    prompt, evidence = build_corpus_prompt(question, hits, pid2fname, max_chars=max_chars)

    answer = call_llm([
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ç§‘ç ”è®ºæ–‡ç»¼è¿°åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt}
    ])

    return answer.strip(), evidence


# æ–¹ä¾¿å‘½ä»¤è¡Œç›´æ¥ç”¨
if __name__ == "__main__":
    print("ğŸ§  è·¨è®ºæ–‡ç§‘ç ”é—®ç­”åŠ©æ‰‹ï¼ˆCorpus RAGï¼‰")
    print("æç¤ºï¼šè¯·ç¡®ä¿å·²ç»è¿è¡Œè¿‡ paper_indexer.py æ„å»ºç´¢å¼•ã€‚\n")

    while True:
        q = input("è¯·è¾“å…¥ä½ çš„ç§‘ç ”é—®é¢˜ï¼ˆæˆ–è¾“å…¥ q é€€å‡ºï¼‰ï¼š").strip()
        if not q or q.lower() == "q":
            break

        print("\nğŸ” æ­£åœ¨æ£€ç´¢è¯­æ–™åº“å¹¶ç”Ÿæˆå›ç­”...\n")
        answer, evidence = corpus_rag_query(q, top_k=12, max_chars=4000)

        print("====== æ¨¡å‹å›ç­” ======\n")
        print(answer)
        print("\n====== è¯æ®æ¥æºï¼ˆè®ºæ–‡+é¡µç ï¼‰======\n")
        for i, e in enumerate(evidence, 1):
            print(f"[{i}] è®ºæ–‡: {e['filename']} | ç¬¬ {e['page']} é¡µ")
        print("\n" + "=" * 50 + "\n")
